{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikav7/TTS/blob/TTS_training/first_training_coqui_tts_vv18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79d99ef",
      "metadata": {
        "id": "f79d99ef"
      },
      "source": [
        "# Train your first üê∏ TTS model üí´\n",
        "\n",
        "### üëã Hello and welcome to Coqui (üê∏) TTS\n",
        "\n",
        "The goal of this notebook is to show you a **typical workflow** for **training** and **testing** a TTS model with üê∏.\n",
        "\n",
        "Let's train a very small model on a very small amount of data so we can iterate quickly.\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Download data and format it for üê∏ TTS.\n",
        "2. Configure the training and testing runs.\n",
        "3. Train a new model.\n",
        "4. Test the model and display its performance.\n",
        "\n",
        "So, let's jump right in!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2aec78",
      "metadata": {
        "id": "fa2aec78"
      },
      "outputs": [],
      "source": [
        "## Install Coqui TTS\n",
        "! pip install -U pip\n",
        "! pip install TTS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be5fe49c",
      "metadata": {
        "id": "be5fe49c"
      },
      "source": [
        "## ‚úÖ Data Preparation\n",
        "\n",
        "### **First things first**: we need some data.\n",
        "\n",
        "We're training a Text-to-Speech model, so we need some _text_ and we need some _speech_. Specificially, we want _transcribed speech_. The speech must be divided into audio clips and each clip needs transcription. More details about data requirements such as recording characteristics, background noise abd vocabulary coverage can be found in the [üê∏TTS documentation](https://tts.readthedocs.io/en/latest/formatting_your_dataset.html).\n",
        "\n",
        "If you have a single audio file and you need to **split** it into clips. It is also important to use a lossless audio file format to prevent compression artifacts. We recommend using **wav** file format.\n",
        "\n",
        "The data format we will be adopting for this tutorial is taken from the widely-used  **LJSpeech** dataset, where **waves** are collected under a folder:\n",
        "\n",
        "<span style=\"color:purple;font-size:15px\">\n",
        "/wavs<br /> \n",
        " &emsp;| - audio1.wav<br /> \n",
        " &emsp;| - audio2.wav<br /> \n",
        " &emsp;| - audio3.wav<br /> \n",
        "  ...<br /> \n",
        "</span>\n",
        "\n",
        "and a **metadata.csv** file will have the audio file name in parallel to the transcript, delimited by `|`: \n",
        " \n",
        "<span style=\"color:purple;font-size:15px\">\n",
        "# metadata.csv <br /> \n",
        "audio1|This is my sentence. <br /> \n",
        "audio2|This is maybe my sentence. <br /> \n",
        "audio3|This is certainly my sentence. <br /> \n",
        "audio4|Let this be your sentence. <br /> \n",
        "...\n",
        "</span>\n",
        "\n",
        "In the end, we should have the following **folder structure**:\n",
        "\n",
        "<span style=\"color:purple;font-size:15px\">\n",
        "/MyTTSDataset <br /> \n",
        "&emsp;| <br /> \n",
        "&emsp;| -> metadata.csv<br /> \n",
        "&emsp;| -> /wavs<br /> \n",
        "&emsp;&emsp;| -> audio1.wav<br /> \n",
        "&emsp;&emsp;| -> audio2.wav<br /> \n",
        "&emsp;&emsp;| ...<br /> \n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69501a10-3b53-4e75-ae66-90221d6f2271",
      "metadata": {
        "id": "69501a10-3b53-4e75-ae66-90221d6f2271"
      },
      "source": [
        "üê∏TTS already provides tooling for the _LJSpeech_. if you use the same format, you can start training your models right away. <br /> \n",
        "\n",
        "After you collect and format your dataset, you need to check two things. Whether you need a **_formatter_** and a **_text_cleaner_**. <br /> The **_formatter_** loads the text file (created above) as a list and the **_text_cleaner_** performs a sequence of text normalization operations that converts the raw text into the spoken representation (e.g. converting numbers to text, acronyms, and symbols to the spoken format).\n",
        "\n",
        "If you use a different dataset format then the LJSpeech or the other public datasets that üê∏TTS supports, then you need to write your own **_formatter_** and  **_text_cleaner_**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f226c8-4e55-48fa-937b-8415d539b17c",
      "metadata": {
        "id": "e7f226c8-4e55-48fa-937b-8415d539b17c"
      },
      "source": [
        "## ‚è≥Ô∏è Loading your dataset\n",
        "Load one of the dataset supported by üê∏TTS.\n",
        "\n",
        "We will start by defining dataset config and setting LJSpeech as our target dataset and define its path.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anyU2zwnPE9f",
        "outputId": "4c78d389-f96f-4de4-cc36-cbd5498d7819"
      },
      "id": "anyU2zwnPE9f",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66",
      "metadata": {
        "id": "b3cb0191-b8fc-4158-bd26-8423c2a8ba66"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/LJSpeech001/output2\"\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pwd"
      ],
      "metadata": {
        "id": "yvyS4khhgjy7"
      },
      "id": "yvyS4khhgjy7",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6b7019-3685-4b48-8917-c152e288d7e3",
      "metadata": {
        "id": "ae6b7019-3685-4b48-8917-c152e288d7e3"
      },
      "outputs": [],
      "source": [
        "#Download the REDUCED LJSpeech dataset. (just first speaker 001 - 186 files)\n",
        "!wget -O $output_path/LJSpeech001.tar.bz2 https://drive.google.com/drive/folders/1UYFVOaKB8jL40qlgs_3h5fBGg4VW7ezF?usp=sharing\n",
        "\n",
        "#!tar -xf $output_path/LJSpeech001.tar.bz2 -C $output_path\n",
        "\n",
        "#with .tar.bz2\n",
        "\n",
        "#DOWNLOAD LJSPEECH DATASET TO TRAIN GLOW-TTS MODEL ON IT\n",
        "#!wget http://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "!wget https://drive.google.com/file/d/1ssehGMDDgNlaEWHmnEsBD0IzXSnTkrL6/view?usp=share_link/LJSpeech001.tar.bz2\n",
        "\n",
        "#DECOMPRESS\n",
        "!tar -xjf LJSpeech001.tar.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5",
      "metadata": {
        "id": "76cd3ab5-6387-45f1-b488-24734cc1beb5"
      },
      "outputs": [],
      "source": [
        "#CONFIGURE DATASET\n",
        "dataset_config = BaseDatasetConfig(\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadatas.csv\", path=os.path.join(output_path, \"/content/drive/MyDrive/LJSpeech001/\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae82fd75",
      "metadata": {
        "id": "ae82fd75"
      },
      "source": [
        "## ‚úÖ Train a new model\n",
        "\n",
        "Let's kick off a training run üöÄüöÄüöÄ.\n",
        "\n",
        "Deciding on the model architecture you'd want to use is based on your needs and available resources. Each model architecture has it's pros and cons that define the run-time efficiency and the voice quality.\n",
        "We have many recipes under `TTS/recipes/` that provide a good starting point. For this tutorial, we will be using `GlowTTS`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f",
      "metadata": {
        "id": "f5876e46-2aee-4bcf-b6b3-9e3c535c553f"
      },
      "source": [
        "We will begin by initializing the model training configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84",
      "metadata": {
        "id": "5483ca28-39d6-49f8-a18e-4fb53c50ad84"
      },
      "outputs": [],
      "source": [
        "# GlowTTSConfig: all model related values for training, validating and testing.\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "config = GlowTTSConfig(\n",
        "    batch_size=32,\n",
        "    eval_batch_size=16,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=3,\n",
        "    text_cleaner=\"phoneme_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    print_step=25,\n",
        "    print_eval=False,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    save_step=1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install cython to prevent errors in the inizialization of the audio processor\n",
        "!pip install -U Cython"
      ],
      "metadata": {
        "id": "BqQrjHeWNMIj"
      },
      "id": "BqQrjHeWNMIj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b93ed377-80b7-447b-bd92-106bffa777ee",
      "metadata": {
        "id": "b93ed377-80b7-447b-bd92-106bffa777ee"
      },
      "source": [
        "Next we will initialize the audio processor which is used for feature extraction and audio I/O."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1b12f61-f851-4565-84dd-7640947e04ab",
      "metadata": {
        "id": "b1b12f61-f851-4565-84dd-7640947e04ab"
      },
      "outputs": [],
      "source": [
        "from TTS.utils.audio import AudioProcessor\n",
        "ap = AudioProcessor.init_from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d461683-b05e-403f-815f-8007bda08c38",
      "metadata": {
        "id": "1d461683-b05e-403f-815f-8007bda08c38"
      },
      "source": [
        "Next we will initialize the tokenizer which is used to convert text to sequences of token IDs.  If characters are not defined in the config, default characters are passed to the config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "014879b7-f18d-44c0-b24a-e10f8002113a",
      "metadata": {
        "id": "014879b7-f18d-44c0-b24a-e10f8002113a"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978",
      "metadata": {
        "id": "df3016e1-9e99-4c4f-94e3-fa89231fd978"
      },
      "source": [
        "Next we will load data samples. Each sample is a list of ```[text, audio_file_path, speaker_name]```. You can define your custom sample loader returning the list of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
      "metadata": {
        "id": "cadd6ada-c8eb-4f79-b8fe-6d72850af5a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a68a8d-7e3e-4327-920b-1eeaa1a6d81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Found 186 files in /content/drive/MyDrive/LJSpeech001\n"
          ]
        }
      ],
      "source": [
        "from TTS.tts.datasets import load_tts_samples\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19",
      "metadata": {
        "id": "db8b451e-1fe1-4aa3-b69e-ab22b925bd19"
      },
      "source": [
        "Now we're ready to initialize the model.\n",
        "\n",
        "Models take a config object and a speaker manager as input. Config defines the details of the model like the number of layers, the size of the embedding, etc. Speaker manager is used by multi-speaker models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4",
      "metadata": {
        "id": "ac2ffe3e-ad0c-443e-800c-9b076ee811b4"
      },
      "outputs": [],
      "source": [
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2832c56-889d-49a6-95b6-eb231892ecc6",
      "metadata": {
        "id": "e2832c56-889d-49a6-95b6-eb231892ecc6"
      },
      "source": [
        "Trainer provides a generic API to train all the üê∏TTS models with all its perks like mixed-precision training, distributed training, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f609945-4fe0-4d0d-b95e-11d7bfb63ebe",
        "outputId": "7c6e1aaf-a777-4c43-e480-75e904d17222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " > Training Environment:\n",
            " | > Num. of CPUs: 2\n",
            " | > Num. of Torch Threads: 1\n",
            " | > Torch seed: 54321\n",
            " | > Torch CUDNN: True\n",
            " | > Torch CUDNN deterministic: False\n",
            " | > Torch CUDNN benchmark: False\n",
            "\n",
            " > Model has 28610257 parameters\n"
          ]
        }
      ],
      "source": [
        "from trainer import Trainer, TrainerArgs\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b320831-dd83-429b-bb6a-473f9d49d321",
      "metadata": {
        "id": "5b320831-dd83-429b-bb6a-473f9d49d321"
      },
      "source": [
        "### AND... 3,2,1... START TRAINING üöÄüöÄüöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4c07f99-3d1d-4bea-801e-9f33bbff0e9f",
        "outputId": "17522a44-9a94-415f-c53f-6ffeedad2aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 0/3\u001b[0m\n",
            " --> /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Pre-computing phonemes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/185 [00:00<00:33,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√∞…õ…π ta…™p …™z …în √∞…ô la…™nz …ôv √∞…ô dÕ° í…öm…ôn √¶nd f…π…õntÕ° É …π√¶√∞…ö √∞…ôn …ôv √∞…ô …πo äm…ôn p…π…™nt…öz.\n",
            " [!] Character 'Õ°' not found in the vocabulary. Discarding it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 185/185 [00:16<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 1 not found characters:\n",
            "\t| > Õ°\n",
            "| > Number of instances : 185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2022-11-24 12:58:33) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " | > Preprocessing samples\n",
            " | > Max text length: 169\n",
            " | > Min text length: 26\n",
            " | > Avg text length: 98.91891891891892\n",
            " | \n",
            " | > Max audio length: 220083.0\n",
            " | > Min audio length: 39347.0\n",
            " | > Avg audio length: 142243.77837837837\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "\n",
            "\u001b[1m   --> STEP: 0/6 -- GLOBAL_STEP: 0\u001b[0m\n",
            "     | > current_lr: 0.00000 \n",
            "     | > step_time: 14.33900  (14.33900)\n",
            "     | > loader_time: 11.20060  (11.20060)\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 1 not found characters:\n",
            "\t| > Õ°\n",
            "| > Number of instances : 1\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 62\n",
            " | > Min text length: 62\n",
            " | > Avg text length: 62.0\n",
            " | \n",
            " | > Max audio length: 81075.0\n",
            " | > Min audio length: 81075.0\n",
            " | > Avg audio length: 81075.0\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time: 0.32831 \u001b[0m(+0.00000)\n",
            "     | > avg_loss: 3.38361 \u001b[0m(+0.00000)\n",
            "     | > avg_log_mle: 0.75623 \u001b[0m(+0.00000)\n",
            "     | > avg_loss_dur: 2.62739 \u001b[0m(+0.00000)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000/best_model_6.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 1/3\u001b[0m\n",
            " --> /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "\u001b[1m > TRAINING (2022-11-24 13:00:07) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 1 not found characters:\n",
            "\t| > Õ°\n",
            "| > Number of instances : 185\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 169\n",
            " | > Min text length: 26\n",
            " | > Avg text length: 98.91891891891892\n",
            " | \n",
            " | > Max audio length: 220083.0\n",
            " | > Min audio length: 39347.0\n",
            " | > Avg audio length: 142243.77837837837\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 1 not found characters:\n",
            "\t| > Õ°\n",
            "| > Number of instances : 1\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 62\n",
            " | > Min text length: 62\n",
            " | > Avg text length: 62.0\n",
            " | \n",
            " | > Max audio length: 81075.0\n",
            " | > Min audio length: 81075.0\n",
            " | > Avg audio length: 81075.0\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[92m 0.15060 \u001b[0m(-0.17771)\n",
            "     | > avg_loss:\u001b[92m 3.37966 \u001b[0m(-0.00395)\n",
            "     | > avg_log_mle:\u001b[92m 0.75288 \u001b[0m(-0.00335)\n",
            "     | > avg_loss_dur:\u001b[92m 2.62678 \u001b[0m(-0.00060)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000/best_model_12.pth\n",
            "\n",
            "\u001b[4m\u001b[1m > EPOCH: 2/3\u001b[0m\n",
            " --> /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000\n",
            "\n",
            "\u001b[1m > TRAINING (2022-11-24 13:02:09) \u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 1 not found characters:\n",
            "\t| > Õ°\n",
            "| > Number of instances : 185\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 169\n",
            " | > Min text length: 26\n",
            " | > Avg text length: 98.91891891891892\n",
            " | \n",
            " | > Max audio length: 220083.0\n",
            " | > Min audio length: 39347.0\n",
            " | > Avg audio length: 142243.77837837837\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[1m > EVALUATION \u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "> DataLoader initialization\n",
            "| > Tokenizer:\n",
            "\t| > add_blank: False\n",
            "\t| > use_eos_bos: False\n",
            "\t| > use_phonemes: True\n",
            "\t| > phonemizer:\n",
            "\t\t| > phoneme language: en-us\n",
            "\t\t| > phoneme backend: gruut\n",
            "\t| > 1 not found characters:\n",
            "\t| > Õ°\n",
            "| > Number of instances : 1\n",
            " | > Preprocessing samples\n",
            " | > Max text length: 62\n",
            " | > Min text length: 62\n",
            " | > Avg text length: 62.0\n",
            " | \n",
            " | > Max audio length: 81075.0\n",
            " | > Min audio length: 81075.0\n",
            " | > Avg audio length: 81075.0\n",
            " | > Num. instances discarded samples: 0\n",
            " | > Batch group size: 0.\n",
            " | > Synthesizing test sentences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
            "     | > avg_loader_time:\u001b[91m 0.20784 \u001b[0m(+0.05725)\n",
            "     | > avg_loss:\u001b[92m 3.37928 \u001b[0m(-0.00038)\n",
            "     | > avg_log_mle:\u001b[92m 0.75287 \u001b[0m(-0.00001)\n",
            "     | > avg_loss_dur:\u001b[92m 2.62641 \u001b[0m(-0.00038)\n",
            "\n",
            " > BEST MODEL : /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000/best_model_18.pth\n"
          ]
        }
      ],
      "source": [
        "trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cff0c40-2734-40a6-a905-e945a9fb3e98",
      "metadata": {
        "id": "4cff0c40-2734-40a6-a905-e945a9fb3e98"
      },
      "source": [
        "#### üöÄ Run the Tensorboard. üöÄ\n",
        "On the notebook and Tensorboard, you can monitor the progress of your model. Also Tensorboard provides certain figures and sample outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec",
      "metadata": {
        "id": "5a85cd3b-1646-40ad-a6c2-49323e08eeec"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboard\n",
        "!tensorboard --logdir=/content/drive/MyDrive/LJSpeech001/output2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f6dc959",
      "metadata": {
        "id": "9f6dc959"
      },
      "source": [
        "## ‚úÖ Test the model\n",
        "\n",
        "We made it! üôå\n",
        "\n",
        "Let's kick off the testing run, which displays performance metrics.\n",
        "\n",
        "We're committing the cardinal sin of ML üòà (aka - testing on our training data) so you don't want to deploy this model into production. In this notebook we're focusing on the workflow itself, so it's forgivable üòá\n",
        "\n",
        "You can see from the test output that our tiny model has overfit to the data, and basically memorized this one sentence.\n",
        "\n",
        "When you start training your own models, make sure your testing data doesn't include your training data üòÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99fada7a-592f-4a09-9369-e6f3d82de3a0",
      "metadata": {
        "id": "99fada7a-592f-4a09-9369-e6f3d82de3a0"
      },
      "source": [
        "Let's get the latest saved checkpoint. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961",
      "metadata": {
        "id": "6dd47ed5-da8e-4bf9-b524-d686630d6961"
      },
      "outputs": [],
      "source": [
        "import glob, os\n",
        "output_path = \"/content/drive/MyDrive/LJSpeech001/output2\"\n",
        "ckpts = sorted([f for f in glob.glob(output_path+\"/*/*.pth\")])\n",
        "configs = sorted([f for f in glob.glob(output_path+\"/*/*.json\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dd42bc7a",
      "metadata": {
        "id": "dd42bc7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbb9faac-1efe-4893-ece9-3b7f41b61ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tts [-h] [--list_models [LIST_MODELS]]\n",
            "           [--model_info_by_idx MODEL_INFO_BY_IDX]\n",
            "           [--model_info_by_name MODEL_INFO_BY_NAME] [--text TEXT]\n",
            "           [--model_name MODEL_NAME] [--vocoder_name VOCODER_NAME]\n",
            "           [--config_path CONFIG_PATH] [--model_path MODEL_PATH]\n",
            "           [--out_path OUT_PATH] [--use_cuda USE_CUDA]\n",
            "           [--vocoder_path VOCODER_PATH]\n",
            "           [--vocoder_config_path VOCODER_CONFIG_PATH]\n",
            "           [--encoder_path ENCODER_PATH]\n",
            "           [--encoder_config_path ENCODER_CONFIG_PATH]\n",
            "           [--speakers_file_path SPEAKERS_FILE_PATH]\n",
            "           [--language_ids_file_path LANGUAGE_IDS_FILE_PATH]\n",
            "           [--speaker_idx SPEAKER_IDX] [--language_idx LANGUAGE_IDX]\n",
            "           [--speaker_wav SPEAKER_WAV [SPEAKER_WAV ...]]\n",
            "           [--gst_style GST_STYLE]\n",
            "           [--capacitron_style_wav CAPACITRON_STYLE_WAV]\n",
            "           [--capacitron_style_text CAPACITRON_STYLE_TEXT]\n",
            "           [--list_speaker_idxs [LIST_SPEAKER_IDXS]]\n",
            "           [--list_language_idxs [LIST_LANGUAGE_IDXS]]\n",
            "           [--save_spectogram SAVE_SPECTOGRAM] [--reference_wav REFERENCE_WAV]\n",
            "           [--reference_speaker_idx REFERENCE_SPEAKER_IDX]\n",
            "           [--progress_bar PROGRESS_BAR]\n",
            "tts: error: unrecognized arguments: /content/drive/MyDrive/LJSpeech001/output2/run-November-24-2022_12+53PM-0000000/best_model_18.pth]\n"
          ]
        }
      ],
      "source": [
        "!tts --text \"Text for TTS\" \\\n",
        "      --model_path $ckpts \\\n",
        "      --config_path $configs \\\n",
        "      --out_path out.wav\n",
        "\n",
        "# !tts --text \"the sound is horrible\" \\\n",
        "#     --model_path /content/drive/MyDrive/LJSpeech001/output/ckpts \\\n",
        "#     --config_path /content/drive/MyDrive/LJSpeech001/output/configs \\\n",
        "#     --out_path /content/out.wav\n",
        "\n",
        "# !tts --text \"the sound is horrible\" \\\n",
        "#     --model_path /content/drive/MyDrive/trainoutput/run/best_model.pth \\\n",
        "#     --config_path /content/drive/MyDrive/trainoutput/run/config.json \\\n",
        "#     --out_path /content/output.wav"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81cbcb3f-d952-469b-a0d8-8941cd7af670",
      "metadata": {
        "id": "81cbcb3f-d952-469b-a0d8-8941cd7af670"
      },
      "source": [
        "## üì£ Listen to the synthesized wave üì£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff",
      "metadata": {
        "id": "e0000bd6-6763-4a10-a74d-911dd08ebcff"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.display.Audio(\"out.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13914401-cad1-494a-b701-474e52829138",
      "metadata": {
        "id": "13914401-cad1-494a-b701-474e52829138"
      },
      "source": [
        "## üéâ Congratulations! üéâ You now have trained your first TTS model! \n",
        "Follow up with the next tutorials to learn more advanced material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950d9fc6-896f-4a2c-86fd-8fd1fcbbb3f7",
      "metadata": {
        "id": "950d9fc6-896f-4a2c-86fd-8fd1fcbbb3f7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}